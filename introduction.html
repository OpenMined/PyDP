<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction To Differential Privacy &mdash; PyDP 1.1.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=7a812f30"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Introduction to PyDP" href="readme.html" />
    <link rel="prev" title="PyDP Documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            PyDP
          </a>
              <div class="version">
                1.1.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction To Differential Privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="#machine-learning-and-data">Machine Learning and Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-is-differential-privacy-so-important">Why is Differential Privacy so important ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#how-is-differential-privacy-implemented">How is Differential Privacy implemented ?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#local-differential-privacy">Local Differential Privacy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#global-differential-privacy">Global Differential Privacy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#formal-definition-of-differential-privacy">Formal Definition Of Differential Privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="#differential-privacy-in-real-world">Differential - Privacy In Real World</a></li>
<li class="toctree-l1"><a class="reference internal" href="#further-reading">Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme.html">Introduction to PyDP</a></li>
<li class="toctree-l1"><a class="reference internal" href="pydp.html">PyDP</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyDP</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction To Differential Privacy</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/introduction.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-to-differential-privacy">
<h1>Introduction To Differential Privacy<a class="headerlink" href="#introduction-to-differential-privacy" title="Permalink to this heading"></a></h1>
</section>
<section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h1>
<p>The era where we are living in is data driven, tons and tons of data are being generated in every second. A lot of this data is being used to improve our own lifestyle - be it recommending the best series to watch after a tiring day at work, suggesting the best gifts to buy when it’s our best friend’s birthday or keeping our birthday party photos sorted so that we can cherish them years later. All big companies are using data to gain insights of their progress which drives their business. Machine Learning has made our life from easy to easier but is it just about improving our lifestyle? This raises a question can machine learning change the way we live ? Can it improve our healthcare? Can ML be friends to those who are lonely and have no one to talk with? The answer is “Yes” and also “No”.</p>
</section>
<section id="machine-learning-and-data">
<h1>Machine Learning and Data<a class="headerlink" href="#machine-learning-and-data" title="Permalink to this heading"></a></h1>
<p>Machine Learning is extensively both data and research driven. The more the data is, better will be the research on that particular topic. Now, all data cannot be released for research, there is a lot of private information which once leaked can be misused. Take for example, to tackle a particular medical problem we need a lot of medical health records. These records are considered as private information as no person would love the fact that her/his medical records are identifiable by anyone on the internet. Hence, these are some real world issues that need immediate solutions but the hands of the researchers are tied due to the unavailability of data. So, is there a solution ?</p>
<p>This is where “Differential Privacy” comes into the picture, a smarter way to a more secure and private AI. According to Andrew Trask, Founder at OpenMined - “Differential Privacy is the process to answer questions or solve problems using the data that we cannot see.” In this way researchers from all over the world can use private data in their research work without identifying the individual.</p>
<figure class="align-center" id="id2">
<img alt="my-picture1" src="https://user-images.githubusercontent.com/19529592/91377299-b58fbf80-e83c-11ea-9b56-a068ea3155c6.png" />
<figcaption>
<p><span class="caption-text">(Privacy Preserving AI (Andrew Trask) | MIT Deep Learning Series )</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="why-is-differential-privacy-so-important">
<h1>Why is Differential Privacy so important ?<a class="headerlink" href="#why-is-differential-privacy-so-important" title="Permalink to this heading"></a></h1>
<p>The aim of any privacy algorithm is to keep one’s private information safe and secured from external attacks. Differential privacy aims to keep an individual’s identity secured even if their data is being used in research. An easy approach to maintain this kind of privacy is “Data Anonymization” which is a process of removing personally identifiable information from a dataset. It is seen that there are cons in following this approach:</p>
<ul class="simple">
<li><p>Anonymizing certain fields may make the entire dataset useless and not fit for any analysis.</p></li>
<li><p>There are related sources or datasets available on the web and by statistically studying both the datasets, an individual can easily be re-identified.</p></li>
<li><p>If the dataset is large, the type of queries that can be drawn from the dataset cannot be predicted. This makes any dataset prone to external attacks.</p></li>
</ul>
<p>Hence, this process is prone to risk and is considered as fundamentally wrong. Netflix once released a challenge for everyone to build up the best recommendation engine. For this they released an anonymized dataset of 100 million movie ratings from half a million users. So, they did not publicly release any data that could lead to the identification of the users.</p>
<figure class="align-center" id="id3">
<img alt="netflix" src="https://user-images.githubusercontent.com/19529592/91381064-14a50280-e844-11ea-9dd0-1af088c3924d.png" />
<figcaption>
<p><span class="caption-text">Image Credits: Secure and Private AI (Udacity)</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Despite the fact that the dataset was anonymized (no username or movie name was released) yet two Researchers at University of Texas released a <a class="reference external" href="https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf">paper</a> where they showed how they have de-anonymized a maximum chunk of the dataset.</p>
<figure class="align-center">
<img alt="research" src="https://user-images.githubusercontent.com/19529592/91381399-ef64c400-e844-11ea-8535-0180f37962de.png" />
</figure>
<p>They scraped the IMDB Website and by statistical analysis on these two datasets, they were able to identify the movie names and also the individual names. Ten years down the line they have published yet another <a class="reference external" href="https://www.cs.princeton.edu/~arvindn/publications/de-anonymization-retrospective.pdf">research paper</a>  where they have reviewed de-anonymization of datasets in the present world. There are other instances too where such attacks have been made which led to the leakage of private information.</p>
<p>Now, that we have learnt how important is “Differential Privacy”, let see how is the Differential Privacy actually implemented.</p>
</section>
<section id="how-is-differential-privacy-implemented">
<h1>How is Differential Privacy implemented ?<a class="headerlink" href="#how-is-differential-privacy-implemented" title="Permalink to this heading"></a></h1>
<p>According to <a class="reference external" href="https://www.microsoft.com/en-us/research/people/dwork">Cynthia Dwork</a>- <em>“Differential privacy” describes a promise, made by a data holder, or curator, to a data subject: “You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available.”</em></p>
<p>Thus this new area of research addresses the paradox of learning nothing about an individual while learning useful information about the population. This is done by sending queries (a function applied to a database) to the data curator (a protocol run by the set of individuals, using the various techniques for secure multiparty protocols). The goal of the curator is to answer all the queries with highest possible accuracy without leaking any individual information using various Differential-Privacy algorithms.</p>
<p>These algorithms add random noise to the queries and to the database. This is done in two ways:</p>
<ul class="simple">
<li><p>Local Differential Privacy</p></li>
<li><p>Global Differential Privacy</p></li>
</ul>
<section id="local-differential-privacy">
<h2>Local Differential Privacy<a class="headerlink" href="#local-differential-privacy" title="Permalink to this heading"></a></h2>
<p>In local differential privacy the random noise is applied at the start of the process(local) level i.e when the data is sent to the data curator/aggregator. If the data is too confidential, generally the data generators do not want to trust the curator and hence add noise to the dataset beforehand. This is adopted when the Data Curator cannot be completely trusted.</p>
<figure class="align-center" id="id4">
<img alt="local" src="https://user-images.githubusercontent.com/19529592/91381482-1e7b3580-e845-11ea-9419-cd6bdbbd9dbf.png" />
<figcaption>
<p><span class="caption-text">Image Credit: Google Images</span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="global-differential-privacy">
<h2>Global Differential Privacy<a class="headerlink" href="#global-differential-privacy" title="Permalink to this heading"></a></h2>
<p>In Global differential privacy the random noise is applied at the global level i.e when the answer to a query is returned to the User. This type of differential privacy is adopted when the Data generators trusts the data curator completely and leaves it to the curator the amount of noise to be added to the results. This type of privacy results is more accurate as it involves lesser noise.</p>
<figure class="align-center" id="id5">
<img alt="global" src="https://user-images.githubusercontent.com/19529592/91381550-4ec2d400-e845-11ea-8f63-b7a3adb3fde8.png" />
<figcaption>
<p><span class="caption-text">Image Credits: Google Images</span><a class="headerlink" href="#id5" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="formal-definition-of-differential-privacy">
<h1>Formal Definition Of Differential Privacy<a class="headerlink" href="#formal-definition-of-differential-privacy" title="Permalink to this heading"></a></h1>
<p>In the book, “<a class="reference external" href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">The Algorithmic Foundations of Differential Privacy</a>” by Cynthia Dwork and Aaron Roth. Differential Privacy is formally defined as:
.. glossary::
<em>A randomized algorithm M with domain N |X| is (ε, δ)-differentially private if for all S ⊆ Range(M) and for all x, y ∈ N |X| such that ∥x − y∥1 ≤ 1:</em></p>
<blockquote>
<div><p><em>Pr[M(x) ∈ S] ≤ exp(ε) Pr[M(y) ∈ S] + δ</em></p>
</div></blockquote>
<p>The Epsilon <em>(ε)</em> and <em>Delta(δ)</em> parameters measure the threshold for leakage.</p>
<ul class="simple">
<li><p>The Epsilon defines how different the actual actual data is from the queried data. If <em>ε=0</em>, exp(<em>ε</em>)=1 which means both the data are equal.</p></li>
<li><p>The Delta is the probability that an information will accidentally be leaked as compared to the value of Epsilon. If  <em>δ=0</em>, that means no data is being leaked.</p></li>
</ul>
<p>This when both Epsilon and Delta is 0, it is called Perfect-Privacy. The values are set in such a way so that the privacy is maintained. This set of values is known as Privacy-Budget.</p>
</section>
<section id="differential-privacy-in-real-world">
<h1>Differential - Privacy In Real World<a class="headerlink" href="#differential-privacy-in-real-world" title="Permalink to this heading"></a></h1>
<p>Differential Privacy ensures privacy of all sorts of data which can be used by anyone to draw insights which can help them run their business. In the present world, Differentially Private Data Analysis is widely used and these are implemented by using various libraries.</p>
<p><a class="reference external" href="https://github.com/OpenMined/PyDP">PyDP</a> by OpenMined is a Python Wrapper for Differential Privacy which allows all sorts of users to use Differential Privacy in their Projects. Apart from this there are various other real-world cases of Differential Privacy from Medical Imaging to Geolocation search. These have been covered in this <a class="reference external" href="https://blog.openmined.org/use-cases-of-differential-privacy">blogpost</a>  by OpenMined.</p>
<p>SOME OTHER LIBRARIES FOR DP</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/opendifferentialprivacy">OpenDp</a> by Harvard University and Microsoft</p></li>
<li><p><a class="reference external" href="https://github.com/IBM/differential-privacy-library">Diffprivlib</a>  by IBM</p></li>
<li><p>Google’s Differential Privacy <a class="reference external" href="https://github.com/google/differential-privacy">Library</a> .</p></li>
</ul>
<p>DIFFERENTIAL PRIVACY IN USE</p>
<p>Top tech companies are using “Differential Privacy” in their day to day business for the privacy of data. Some of the use cases are here as follows:</p>
<ul class="simple">
<li><p>Uber</p></li>
</ul>
<p>Uber, a popular ride-sharing company uses Differential Privacy in its practices. The company uses a method of Differential Privacy called “<a class="reference external" href="https://github.com/uber-archive/sql-differential-privacy">elastic sensitivity</a>”, developed in the University of California at Berkeley. It uses mathematics to set limits on the number of statistical queries  the staff can conduct on traffic patterns and driver’s revenue. This method also ensures addition of noise in case the potential of a privacy breach is more severe.</p>
<ul class="simple">
<li><p>Apple</p></li>
</ul>
<p>Apple also makes use of differential privacy to analyse user behaviour and improve user experience. Accessing private data such as browsing history, apps that we browse, words that we type etc can compromise user privacy. But these data are extremely useful when it comes to improving user experience. Apple makes use of “<a class="reference external" href="https://machinelearning.apple.com/research/learning-with-privacy-at-scale">Local Differential Privacy</a>” algorithms which ensures that the raw data is randomized before sending it to the servers. This approach is implemented at scale across on millions of users and by harnessing this data various business decisions are taken.</p>
<ul class="simple">
<li><p>Google</p></li>
</ul>
<p>Google also uses this novel approach to keep user data private to themselves and perform data analysis with that data to drive some of their core products. One such product is the Gboard (Google Keyboard), where it uses private data of the user to generate word suggestions. The method used is “Federated Learning” which decreases the reliance on the cloud and puts a strong focus on a user’s privacy. Rather than sending encrypted data to the servers, it downloads the current model on device and improves it by learning from the data on device. The updated model with the changes is sent to the cloud using encrypted communication. This is done at scale across all users and the updates from each user is immediately averaged with other updates to improve the shared model. In the year 2019, <a class="reference external" href="https://developers.googleblog.com/2019/09/enabling-developers-and-organizations.html">Google open sourced the Differential Privacy  library</a>
for others to use.</p>
<p>Differential Privacy is playing an important role in building Privacy-protected Machine Learning solutions. PyDP is an effort to democratize this field. To know more about Differential Privacy and PyDP head over to our amazing blog series at <a class="reference external" href="https://blog.openmined.org">OpenMined Blog</a>.</p>
</section>
<section id="further-reading">
<h1>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://www.udacity.com/course/secure-and-private-ai--ud185">Secure and Private AI Course on Udacity by Andrew Trask</a></p></li>
<li><p><a class="reference external" href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">“The Algorithmic Foundations of Differential Privacy” by Cynthia Dwork and Aaron Roth</a></p></li>
<li><p><a class="reference external" href="https://blog.openmined.org/tag/differential-privacy">OpenMined Blogs on Differential Privacy</a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="PyDP Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="readme.html" class="btn btn-neutral float-right" title="Introduction to PyDP" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, OpenMined.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>